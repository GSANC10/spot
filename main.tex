\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{float}
\usepackage[table]{xcolor}


% for python code
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{teal},
    showstringspaces=false,
    frame=single,
    breaklines=true,
    columns=fullflexible
}


\title{Spotify Track Popularity Predicition}
\author{Kevin Alvarez \and 
Will Burns \and 
Jenna Jabourian \and 
Connor Perrone \and 
Gabe Sanchez}

\begin{document}
\maketitle

\begin{abstract}
Blah Blah Blah
\end{abstract}

\section{Introduction}

The purpose of this project is to analyze the Spotify track data set and develop a model capable of predicting the popularity of the track using various features. The data set contains numerical and categorical attributes that describe musical characteristics such as danceability, loudness, energy, tempo, key, and valence. Our goal is to explore which features are the most strongly correlated with popularity and to compare different predictive methods in terms of accuracy, interpret-ability, and robustness. 

\section{Results}
Blah Blah Blah

\section{Discussion}

Blah Blah Blah

\appendix
\section{Methods}
\textbf{Aside:}, note that all of the code for the following has been already submitted as part of the Project Check-In, therefore does not need to resubmitted according to the project spec. 
\subsection{Exploratory Data Analysis}

For our exploratory data analysis (EDA), we began by examining the structure of the Spotify dataset, including the number of observations, feature types, and any missing or duplicated entries. We generate summary statistics for numerical features to understand their ranges, distributions, and potential outliers.

As part of our exploratory data analysis, we generated histogram plots for several of the major audio features in the dataset, including danceability, energy, loudness, valence, and tempo. These histograms (Figure~\ref{fig:eda_histogram}) helped us understand how each feature is distributed across the dataset and revealed that many musical characteristics have non-uniform or skewed distributions. Understanding these distributions is important for constructing a similarity-based recommendation system, as features with narrow or concentrated ranges may have less discriminative power when comparing tracks.

\begin{figure}[H]
    \centering
    \includegraphics[width=7cm]{pic/histogram_popularity.png}
    \caption{Histogram distributions of selected numerical audio features in the Spotify dataset.}
    \label{fig:eda_histogram}
\end{figure}


To better understand how listener preferences vary across musical categories, we examined the relationship between genre and popularity. The bar chart in Figure~\ref{fig:eda_relation} shows the average popularity for each genre represented in the dataset. This visualization highlights which genres tend to receive higher user engagement on Spotify and also illustrates the uneven distribution of popularity across categories. Observing this imbalance further supports our decision to exclude popularity as a feature for recommendation, as it reflects platform-wide trends rather than individual user taste.

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/relation_genre_pop.png}
    \caption{Relationship between Genre and Popularity.}
    \label{fig:eda_relation}
\end{figure}

To investigate how the audio features relate to one another, we generated a correlation heatmap, shown in Figure~\ref{fig:eda_correlation}. This visualization highlights the strength and direction of pairwise relationships between numerical features in the dataset. Several strong correlations are immediately apparent, such as the positive relationship between loudness and energy, as well as between valence and danceability. These patterns indicate that certain musical characteristics tend to co-occur across tracks. Understanding these relationships is important for a similarity-based recommendation system, since highly correlated features contribute similar information when comparing songs.

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/correlation_heatmap.jpg}
    \caption{Correlation heatmap of numerical Spotify audio features.}
    \label{fig:eda_correlation}
\end{figure}

To further explore the relationships identified in the correlation heatmap, we created scatterplots for pairs of features that exhibited strong positive correlations. Figures~\ref{fig:eda_loudness_v_energy} and~\ref{fig:eda_valence_v_danceability} show two such examples: loudness versus energy, and valence versus danceability. Both plots reveal clear upward trends, indicating that tracks with higher loudness generally have higher energy levels, and songs with greater valence tend to be more danceable. These visual patterns reinforce the idea that certain audio characteristics naturally cluster together, which is valuable information when measuring similarity between songs in a recommendation system.

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/loudness_vs_energy.jpg}
    \caption{Scatterplot of Loudness versus Energy.}
    \label{fig:eda_loudness_v_energy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/valence_vs_danceability.jpg}
    \caption{Scatterplot of Valance verses Danceability.}
    \label{fig:eda_valence_v_danceability}
\end{figure}

\subsection{Data Pre-processing and Feature Engineering}

Before building our recommendation system, we performed several preprocessing steps to ensure that the data set was clean, consistent, and suitable for feature-based similarity comparisons. We started by checking for duplicated songs using the \texttt{track\_id} column. The data set contained a substantial number of duplicates, so we removed all repeated entries by keeping only the first occurrence of each unique \texttt{track\_id}. This reduced the data set from 114,000 rows to 89,741 unique tracks, ensuring that no song was overly tagged during model development.

\begin{lstlisting}[language=Python, caption={Duplicate detection and cleanup}]
dupe_mask = df['track_id'].duplicated(keep=False)
dupes = df[dupe_mask].sort_values('track_id')
df = df.drop_duplicates(subset='track_id', keep='first')
\end{lstlisting}

Next, we removed columns that did not contribute meaningful musical information. Specifically, we dropped the \texttt{track\_id} and \texttt{Unnamed: 0} columns, since they are identifier fields rather than audio or metadata features and therefore cannot be used to compute similarity between songs.
\begin{lstlisting}[language=Python, caption={Column Removal}]
df = df.drop('track_id', axis=1)
df = df.drop('Unnamed: 0', axis=1)
\end{lstlisting}

After cleaning the dataset, we evaluated the remaining features for their suitability in a recommendation setting. Since popularity does not reflect a user’s personal preference and is computed using an opaque algorithm specific to Spotify, we excluded this column from all downstream analysis. This ensures that recommendations are based purely on the audio characteristics of songs rather than global listening trends.

Although our project did not require creating new engineered features, the preprocessing steps above established a consistent and reliable feature set that will later be standardized. 


\subsection{Regression Analysis}

In order to understand how individual audio features contribute to the overall musical characteristics of a track, we applied regression analysis using \texttt{energy} as the target variable. Energy is a continuous numeric feature that reflects the perceptual intensity of a song, making it a good candidate for examining linear relationships in the dataset.

We first computed the correlation between energy and the other numerical features to identify potential predictors. Loudness showed the strongest correlation with energy, followed by danceability and valence. These correlations provided an initial indication that a linear model might capture part of the relationship between audio features and energy.

To quantify these relationships, we trained a multiple linear regression model using seven audio features as predictors. The model was trained on 80\% of the data and evaluated on the remaining 20\%. The resulting validation metrics indicated that the linear model explained a moderate portion of the variance in energy, with R\textsuperscript{2}, MAE, and RMSE values suggesting that the model was capturing general trends but not all complexities of the feature space. Figure~\ref{fig:regression_scatter} shows a scatterplot comparing the model’s predicted energy values to the actual values in the validation set. The clustering around the diagonal line indicates reasonable predictive performance, though with visible deviation reflecting non-linear structure in the data.

We also examined the learned regression coefficients to interpret feature importance. The magnitude and sign of the coefficients aligned with the earlier correlation analysis: loudness had the strongest positive effect on energy, supporting the idea that louder tracks tend to feel more energetic.

To evaluate whether regularization was necessary, we trained a Ridge regression model with an \(\alpha\) value of 2.5. Ridge regression slightly reduced overfitting, bringing the training and validation R\textsuperscript{2} scores closer together. However, the performance improvements were small, indicating that multicollinearity among our selected features was present but not severe. This suggests that while regularization can stabilize the model, the linear feature set did not require heavy penalization to perform effectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{pic/predicted_vs_actual_energy.png}
    \caption{Predicted versus actual energy values for the linear regression model on the validation set. The diagonal dashed line represents perfect predictions.}
    \label{fig:regression_scatter}
\end{figure}

\subsection{Logistic Regression}

To further analyze classification patterns within the dataset, we applied logistic regression to predict whether a song contains explicit content. The target variable \texttt{explicit} is binary, making logistic regression a natural choice for this task. We used eight audio and metadata features, including popularity, danceability, loudness, energy, instrumentalness, speechiness, valence, and tempo, as predictors.

A key challenge in this task is that explicit songs make up a small minority of the data set, resulting in a class imbalance. To address this, we assigned higher weights to the minority (explicit) class during training, allowing the model to better identify explicit tracks without being overwhelmed by the majority class.

After training the model in an 80/20 train–validation split, we evaluated performance using a confusion matrix (Figure~\ref{fig:log_confusion}), classification report metrics and general prediction accuracy. The confusion matrix revealed that the model correctly identified a large proportion of non-explicit songs while achieving reasonable true positive performance on explicit songs, despite the imbalance. This demonstrates that class weighting improved the model’s sensitivity to explicit content.

To further assess predictive quality, we examined the model’s ROC curve and computed the area under the curve (AUC), shown in Figure~\ref{fig:log_roc}. The resulting AUC score indicated that the logistic model performed substantially better than random guessing and was effective at separating explicit from non-explicit songs based on the selected features.

Finally, we performed 5-fold stratified cross-validation to verify the stability of the model. The AUC and accuracy scores across folds remained consistent, indicating that the model generalizes reliably and is not overfitting. Regularization did not significantly impact performance, suggesting that multicollinearity among the selected features was limited, and the logistic model remained stable without heavy penalization, similar to Linear Regression. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{pic/log_confusion_matrix.png}
    \caption{Confusion matrix for logistic regression model predicting explicit content.}
    \label{fig:log_confusion}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{pic/log_roc_curve.png}
    \caption{ROC curve for logistic regression on the validation set. The dashed line represents random guessing.}
    \label{fig:log_roc}
\end{figure}

\subsection{KNN, Decision Trees, and Random Forest}
\textbf{NOTE: i didn't do decision tree / random forest -kevin} \vspace{0.5em}

We applied the K-Nearest Neighbors (KNN) classifier to predict whether a song contains explicit content, using the same set of audio and metadata features as in the logistic regression analysis. Because KNN relies on distance computations, we standardized all feature values prior to training to ensure that no single feature dominated the Euclidean distance metric.

The model was first trained using \( k = 3 \), which provided a good balance between model flexibility and stability. After fitting the classifier on the training set, we evaluated performance on the validation set. The confusion matrix shown in Figure~\ref{fig:knn_confusion} reveals that the KNN model achieved strong performance on the majority (non-explicit) class, with a high true negative rate of 0.9709. However, the model struggled to correctly identify explicit songs, resulting in a lower true positive rate of 0.2876. This imbalance is expected, as the dataset contains far fewer explicit songs and KNN tends to favor the majority class when class densities differ. 

Despite this, the overall prediction accuracy was high (91.31\%), although this figure alone is misleading due to the strong class imbalance. To better assess the classifier’s discriminative ability, we computed the ROC curve and measured the area under the curve (AUC), shown in Figure~\ref{fig:knn_roc}. The resulting AUC score indicated moderate performance, reflecting that while KNN is effective at identifying non-explicit songs, it is less capable of distinguishing explicit content based solely on the available features.

To evaluate the stability of the model, we conducted 5-fold stratified cross-validation using \( k = 5 \). The resulting accuracy and AUC scores were consistent across folds, indicating that the classifier generalizes reliably. However, cross-validation also confirmed that the low recall on the explicit class is inherent to the dataset’s imbalance rather than an artifact of a particular train–test split.

Table~\ref{tab:knn_cv} summarizes the 5-fold cross-validation accuracy and AUC values for the KNN classifier, demonstrating stable performance across folds.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Fold} & \textbf{Accuracy} & \textbf{AUC} \\
\hline
1 & 0.914 & 0.740 \\
2 & 0.909 & 0.709 \\
3 & 0.906 & 0.687 \\
4 & 0.914 & 0.754 \\
5 & 0.914 & 0.715 \\
\hline
\textbf{Mean} & \textbf{0.912} & \textbf{0.721} \\
\hline
\end{tabular}
\caption{5-fold cross-validation results for the KNN classifier.}
\label{tab:knn_cv}
\end{table}

Overall, KNN provided reasonable baseline performance but was limited in detecting minority-class instances. This suggests that distance-based methods may not be ideal for this task unless additional techniques—such as oversampling, synthetic data generation, or alternative distance metrics—are introduced.

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/knn_confusion_matrix.png}
    \caption{Confusion matrix for the KNN classifier ($k=3$) on the validation set.}
    \label{fig:knn_confusion}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/knn_roc_curve.png}
    \caption{ROC curve for the KNN classifier on the validation set. The dashed line represents random guessing.}
    \label{fig:knn_roc}
\end{figure}


\subsection{PCA and Clustering}
% working on this rn -kevin
To explore groupings and latent structure within the Spotify dataset, we applied Principal Component Analysis (PCA) and K-Means clustering. Since clustering algorithms are sensitive to feature scale, we first standardized all numerical predictor variables using a \texttt{StandardScaler}. This ensured that features measured on different scales contributed equally to the distance computations used by K-Means.

We then applied the K-Means algorithm to the standardized feature set. Because the optimal number of clusters is not known beforehand, we used two diagnostic tools: the elbow method and silhouette scores. The elbow plot (Figure~\ref{fig:elbow_plot}) shows the distortion, or within-cluster sum of squares, for values of \( k \) ranging from 2 to 10. The point at which the curve begins to flatten—commonly referred to as the “elbow” suggests a suitable choice for \( k \). Similarly, the silhouette score plot (Figure~\ref{fig:silhouette_plot}) measures how well-separated the clusters are for each value of \( k \). Higher silhouette scores indicate more coherent and better-defined clusters.

Both diagnostics suggested that \( k = 6 \) provided a good balance between low distortion and high separation quality. After selecting \( k = 6 \), we fit the final K-Means model and computed cluster assignments for all standardized samples. Examining the cluster sizes confirmed that no cluster was excessively small or disproportionately large.

To visualize the clusters in two dimensions, we applied PCA to the standardized feature matrix and extracted the first two principal components, which captured the largest amount of variance. Figure~\ref{fig:pca_clusters} shows a 2D scatterplot of the samples in PCA space, colored according to their cluster label. Although PCA reduces the dimensionality and may not retain all information, the plot reveals meaningful separation between several clusters, indicating that the underlying audio features contain structure that can be grouped into musically distinct categories.

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/elbow_plot.png}
    \caption{Elbow method showing distortion values for $k$ between 2 and 10. The point where the curve begins to flatten suggests a suitable number of clusters.}
    \label{fig:elbow_plot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/silhouette_scores.png}
    \caption{Silhouette scores for $k$ between 2 and 10. Higher values indicate more well-defined clusters.}
    \label{fig:silhouette_plot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/pca_clusters.png}
    \caption{K-Means clustering visualized in the first two principal components. Colors represent the six cluster assignments.}
    \label{fig:pca_clusters}
\end{figure}



\subsection{Neural Network Experiments}

To capture nonlinear relationships among audio features and generate meaningful low-dimensional representations of songs, we implemented a neural network autoencoder. The autoencoder learns a compressed embedding of each track by training the network to reconstruct the original input from a reduced latent space. This approach is particularly valuable for recommendation systems, as songs that produce similar embeddings tend to share similar musical characteristics across multiple dimensions.

Before training, we standardized all numerical features in the dataset to ensure consistent scaling across input dimensions. The autoencoder architecture consisted of an encoder with two fully connected layers that reduced the input vector to a 12-dimensional embedding, followed by a symmetric decoder responsible for reconstructing the original feature vector. ReLU activations were used throughout the network, and training was performed for 40 epochs using the Adam optimizer with a learning rate of 0.001 and mean squared error (MSE) as the loss function.

Figure~\ref{fig:auto_loss} shows the error-versus-epoch curve. The steady decline in reconstruction loss, decreasing from approximately 0.10 in the first epoch to below 0.006 by epoch 40, demonstrates stable convergence and effective learning. Table~\ref{tab:autoencoder_metrics} summarizes the reconstructed performance metrics, including MSE, MAE, RMSE, and the R\textsuperscript{2} score. These metrics indicate that the autoencoder achieved exceptionally high reconstruction fidelity, with an R\textsuperscript{2} value of 0.9944 and a very low reconstruction error across all measures.

To provide further detail on the autoencoder’s learning progression, Table~\ref{tab:autoencoder_loss_color} reports the epoch-by-epoch training loss values in a color-coded side-by-side format (Epochs 1–20 and Epochs 21–40). The progressive lightening of cell colors visually reinforces the monotonic decrease in training loss and highlights the network’s consistent improvement throughout training.

After training, we used the learned embeddings to construct a similarity-based recommendation system. For any selected track, we computed its latent embedding and measured cosine similarity against all other embeddings to identify the most similar songs. Because the embedding space reflects nonlinear relationships among multiple audio features, this method naturally identifies songs with similar musical structure, rather than relying on single-feature comparisons.

Overall, the autoencoder proved to be an effective tool for extracting compact, information-rich song representations. These latent embeddings provide a strong foundation for music recommendation and complement the insights gained from earlier regression, classification, and clustering analyses.


\begin{table}[H]
\centering

% Define a custom light-blue color scale
\definecolor{lightblue}{RGB}{224,243,255}

% Helper command to color cells based on loss magnitude
\newcommand{\losscolor}[1]{%
    \pgfmathparse{min(1, (#1/0.10347))}%
    \edef\percent{\pgfmathresult}%
    \cellcolor{blue!\percent!white}{#1}%
}

% Left Table: Epochs 1–20
\begin{minipage}{0.45\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline
\rowcolor{lightgray}
\textbf{Epoch} & \textbf{Loss} \\
\hline
1  & \cellcolor{blue!100!white}{0.10347} \\
2  & \cellcolor{blue!20!white}{0.02057} \\
3  & \cellcolor{blue!14!white}{0.01472} \\
4  & \cellcolor{blue!11!white}{0.01218} \\
5  & \cellcolor{blue!10!white}{0.01088} \\
6  & \cellcolor{blue!9!white}{0.01016} \\
7  & \cellcolor{blue!9!white}{0.00970} \\
8  & \cellcolor{blue!8!white}{0.00913} \\
9  & \cellcolor{blue!8!white}{0.00871} \\
10 & \cellcolor{blue!8!white}{0.00827} \\
11 & \cellcolor{blue!7!white}{0.00795} \\
12 & \cellcolor{blue!7!white}{0.00766} \\
13 & \cellcolor{blue!7!white}{0.00735} \\
14 & \cellcolor{blue!7!white}{0.00717} \\
15 & \cellcolor{blue!7!white}{0.00705} \\
16 & \cellcolor{blue!7!white}{0.00709} \\
17 & \cellcolor{blue!7!white}{0.00681} \\
18 & \cellcolor{blue!7!white}{0.00675} \\
19 & \cellcolor{blue!7!white}{0.00672} \\
20 & \cellcolor{blue!6!white}{0.00653} \\
\hline
\end{tabular}
\end{minipage}
\hfill
% Right Table: Epochs 21–40
\begin{minipage}{0.45\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline
\rowcolor{lightgray}
\textbf{Epoch} & \textbf{Loss} \\
\hline
21 & \cellcolor{blue!6!white}{0.00651} \\
22 & \cellcolor{blue!6!white}{0.00649} \\
23 & \cellcolor{blue!6!white}{0.00639} \\
24 & \cellcolor{blue!6!white}{0.00630} \\
25 & \cellcolor{blue!6!white}{0.00626} \\
26 & \cellcolor{blue!6!white}{0.00623} \\
27 & \cellcolor{blue!6!white}{0.00613} \\
28 & \cellcolor{blue!6!white}{0.00611} \\
29 & \cellcolor{blue!6!white}{0.00611} \\
30 & \cellcolor{blue!6!white}{0.00600} \\
31 & \cellcolor{blue!6!white}{0.00593} \\
32 & \cellcolor{blue!6!white}{0.00585} \\
33 & \cellcolor{blue!6!white}{0.00582} \\
34 & \cellcolor{blue!6!white}{0.00575} \\
35 & \cellcolor{blue!5!white}{0.00567} \\
36 & \cellcolor{blue!6!white}{0.00574} \\
37 & \cellcolor{blue!5!white}{0.00555} \\
38 & \cellcolor{blue!5!white}{0.00550} \\
39 & \cellcolor{blue!5!white}{0.00549} \\
40 & \cellcolor{blue!5!white}{0.00545} \\
\hline
\end{tabular}
\end{minipage}

\caption{Side-by-side comparison of autoencoder training loss across 40 epochs. Darker shades represent higher loss values and lighter shades represent lower loss values.}
\label{tab:autoencoder_loss_color}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
MSE  & 0.005568 \\
MAE  & 0.041211 \\
RMSE & 0.074618 \\
R\textsuperscript{2} & 0.994432 \\
\hline
\end{tabular}
\caption{Autoencoder reconstruction performance metrics.}
\label{tab:autoencoder_metrics}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{pic/autoencoder_loss.png}
    \caption{Training loss curve for the autoencoder over 40 epochs.}
    \label{fig:auto_loss}
\end{figure}


\subsection{Hyperparameter Tuning}

Throughout the project, we performed hyperparameter tuning across several models in order to improve performance, ensure model stability, and validate that our chosen configurations generalized well beyond a single train–test split. Each algorithm required different hyperparameters to be explored, and the decisions were guided by empirical results, diagnostic plots, and cross-validation procedures.

\subsubsection*{Clustering (K-Means)}
One of the most important hyperparameters in K-Means clustering is the number of clusters \( k \). To determine an appropriate value, we tested values of \( k \) ranging from 2 to 10 and used two evaluation methods:
\begin{itemize}
    \item \textbf{Elbow Method:} We plotted distortion (within-cluster sum of squares) across different values of \( k \) and observed where the curve began to flatten.
    \item \textbf{Silhouette Scores:} We computed silhouette coefficients for each \( k \) to measure cluster cohesion and separation.
\end{itemize}
Both diagnostics indicated that \( k = 6 \) provided a strong balance between separation quality and cluster compactness, and this value was selected for the final clustering model.

\subsubsection*{Linear and Regularized Regression}
For linear regression, the primary goal was to analyze which features contributed significantly to predicting track energy. No regularization was applied in the standard model, allowing the coefficients to reflect the natural linear relationships in the dataset.

We then tuned the Ridge regression model by adjusting the regularization strength parameter \( \alpha \). Several values were tested, and \( \alpha = 2.5 \) produced the best tradeoff between reducing coefficient variance and maintaining predictive performance. Ridge demonstrated only modest improvements over standard linear regression, suggesting low multicollinearity among features.

\subsubsection*{Logistic Regression}
The logistic regression model required tuning primarily due to the strong class imbalance between explicit and non-explicit tracks. We adjusted the following hyperparameters:

\begin{itemize}
    \item \textbf{Class Weights:} To increase sensitivity to explicit tracks, we applied class weights of \texttt{\{0:1, 1:8\}}, substantially improving recall for the minority class.
    \item \textbf{Maximum Iterations:} We increased \texttt{max\_iter} to 500 to ensure convergence with weighted training.
\end{itemize}

After tuning, the logistic regression model achieved stable AUC and accuracy metrics, confirmed through 5-fold stratified cross-validation.

\subsubsection*{K-Nearest Neighbors}
KNN performance depends heavily on the choice of \( k \). We initially trained the model with \( k = 3 \), which provided a good baseline. To evaluate model stability, we ran stratified 5-fold cross-validation with \( k = 5 \), observing consistent accuracy and AUC values across folds. This indicated that the classifier was robust to moderate changes in \( k \), although limited by class imbalance inherent in the dataset.

\subsubsection*{Neural Network Autoencoder}
The autoencoder required tuning of several hyperparameters, including:

\begin{itemize}
    \item \textbf{Embedding Dimension:} We selected a 12-dimensional latent space to balance compression and reconstruction quality.
    \item \textbf{Hidden Layer Widths:} The encoder and decoder each used a 64-unit hidden layer based on preliminary experiments.
    \item \textbf{Activation Functions:} ReLU was chosen for its stability and suitability for continuous-valued data.
    \item \textbf{Learning Rate:} A learning rate of 0.001 with the Adam optimizer produced smooth convergence.
    \item \textbf{Epochs:} Training for 40 epochs allowed the loss to stabilize without overfitting, as seen in the downward trend in Table~3 and Figure~\ref{fig:auto_loss}.
    \item \textbf{Batch Size:} A batch size of 64 balanced learning stability and computational efficiency.
\end{itemize}

The combination of these tuned hyperparameters yielded a highly accurate autoencoder with excellent reconstruction performance, as summarized in Table~2.

\subsubsection*{Summary}
Across all models, hyperparameter tuning played a key role in improving predictive accuracy, stability, and interpretability. By systematically adjusting cluster sizes, regularization strengths, class weights, nearest-neighbor counts, and neural network architecture components, we ensured that each modeling approach was optimized for the characteristics of the Spotify dataset.





% \bibliographystyle{alpha}
 % \bibliography{sample}

\end{document}
